//
//    rfnoc-hls-neuralnet: Vivado HLS code for neural-net building blocks
//
//    Copyright (C) 2017 EJ Kreinar
//
//    This program is free software: you can redistribute it and/or modify
//    it under the terms of the GNU General Public License as published by
//    the Free Software Foundation, either version 3 of the License, or
//    (at your option) any later version.
//
//    This program is distributed in the hope that it will be useful,
//    but WITHOUT ANY WARRANTY; without even the implied warranty of
//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//    GNU General Public License for more details.
//
//    You should have received a copy of the GNU General Public License
//    along with this program.  If not, see <http://www.gnu.org/licenses/>.
//
#include <iostream>
#include <mc_scverify.h>

#include "myproject.h"
#include "parameters.h"

// Weights start

fc1_weight_t w2[320] = {-0.0627525821, -0.0244829990, -0.0504210852, 0.0767213628, 0.0266646538, -0.0223473068, 0.0589016974, 0.0179401711, -0.0008565038, 0.0197155084, 0.0140037490, -0.0670356378, -0.0000389121, 0.0309973359, 0.0433969423, -0.0328318588, -0.0078112669, -0.0298973415, 0.0228517335, -0.0260933731, -0.0089888088, 0.0606180392, -0.0268745553, 0.0083980793, -0.0195349064, 0.0368514955, -0.0120926946, 0.1034605056, -0.0279838331, 0.0334335901, 0.0675059855, -0.0589992292, -0.0083870003, -0.0180075634, 0.0194317196, -0.0004238680, 0.1010927707, 0.0266538113, 0.0573373511, 0.0153914718, -0.0594660938, -0.0548413396, -0.0833949670, 0.0318752825, -0.0225188192, -0.0187593363, 0.1285061985, -0.0295760613, 0.0405660346, 0.0125041949, -0.0637898147, -0.1469158232, -0.0821728334, 0.0853646472, -0.0354093872, -0.0526277833, 0.0140459090, 0.0462820642, -0.0850728601, 0.0585811213, -0.1265024096, -0.0527646318, 0.0609523356, -0.0788403749, 0.0177744646, -0.0330550484, 0.0149336131, -0.0181673970, 0.0016585405, -0.0761195794, -0.0662097186, -0.0327839851, 0.0209485721, 0.0536301099, -0.0972655416, 0.0698096529, -0.0290447306, 0.0183740277, 0.0248215105, -0.0509940498, 0.0437421128, 0.0650432631, -0.0617076829, -0.0341471061, 0.0248517189, -0.0484742261, -0.0038279314, -0.0968075618, 0.0286687147, -0.0766341686, -0.0436332971, -0.0147621622, 0.0155136576, -0.0703829154, 0.1343460530, 0.0514828451, 0.0412775204, 0.0724244341, 0.0487861969, 0.0435175188, 0.0736121386, -0.0190436598, -0.0287749711, 0.0003127823, 0.0040258686, -0.0460362956, 0.0185488947, 0.0506341122, 0.0148400767, -0.0262655262, 0.0119258882, 0.0176182613, -0.1063064262, -0.0017621234, 0.0790898949, 0.0757892355, 0.0463618822, -0.0434892178, -0.0056605604, 0.0478286184, 0.0692863539, -0.0069921426, -0.0560109876, 0.0413774364, -0.0306284819, 0.0317604728, 0.0175436679, 0.0605031326, -0.0086490372, -0.0042818733, 0.0600749552, -0.0957239196, -0.0000858482, 0.0258909445, 0.0607380159, -0.0563195124, 0.0385334305, 0.0586599372, -0.0721005648, -0.0145448148, -0.0802360699, 0.0775460079, 0.0175106339, -0.0227186363, -0.0153432330, -0.0056865588, 0.0204456151, -0.0475241803, 0.0072268280, 0.0596509762, 0.0170927141, 0.0053422451, 0.0757952854, 0.0143749416, -0.0494399779, -0.0144065535, 0.0807171986, 0.0583927818, -0.0390287712, -0.0133727705, 0.0243531726, 0.0029882158, -0.0042192852, -0.0100186104, -0.0269058198, 0.0037641160, 0.0466539599, -0.0773638710, -0.0201068353, -0.0013660303, 0.0437777005, 0.0003128911, -0.0384215564, 0.0154071301, -0.0060283458, -0.0404847674, 0.0718261451, 0.0049356120, -0.0643484890, 0.0505819805, -0.0642611310, 0.0008037542, 0.0246701967, 0.1557042599, -0.0297115929, 0.0037274279, 0.1262534708, 0.0071009295, -0.1110485196, -0.0210202057, -0.0732830167, 0.0040199379, -0.0404384993, 0.0156238703, -0.0286757797, -0.0150828110, -0.0679196715, -0.0327439606, 0.0029247694, 0.0311939623, 0.1089829579, 0.0089163221, 0.0184771717, 0.0020969801, 0.0389723368, 0.0321618430, -0.0575593524, -0.0595845766, 0.0533839837, -0.1082911417, -0.0416780375, 0.0485215634, -0.0312396735, -0.0154496925, -0.0300665181, -0.0720901415, 0.1047358066, 0.0025486727, -0.0387417562, -0.0015657976, 0.0166098960, -0.0918352529, 0.0998590142, -0.0222117435, -0.0159220155, -0.0311378539, -0.0352018289, -0.0441039018, 0.0505967923, -0.0359165668, -0.0036702019, -0.0004682567, -0.0693418309, 0.0522050038, -0.0016688748, -0.0932899714, 0.0245190691, 0.0059424024, 0.0128986137, -0.0112773431, -0.0187067129, 0.0545904413, -0.0119061312, 0.0640640408, -0.0216393508, -0.1286233068, -0.0753883719, -0.0170413349, 0.0328091979, 0.0922169834, 0.0396489017, 0.0169149563, -0.0302541982, -0.0186162479, -0.0220699385, 0.0232710447, 0.0502010584, -0.0944049507, -0.0716946349, -0.0766161308, 0.0469474196, -0.0041213147, 0.0680030808, 0.0998763666, -0.0834167302, -0.0864545628, 0.0959230289, 0.0576892495, 0.0277029034, 0.0847099945, -0.0581009798, 0.0243976135, -0.0313302912, 0.0313001201, 0.0214435309, -0.0027560685, -0.0134292515, -0.0206562225, -0.0325222276, -0.0345526896, -0.0567457862, 0.0107073877, 0.0765828118, 0.0159037616, 0.0049932287, 0.0814462528, -0.0292379986, -0.0087122833, -0.0603120402, 0.0816995725, -0.0067470181, 0.0224803835, -0.0238546748, 0.0190072767, -0.0603399538, -0.0217764601, -0.0308746193, 0.0937770456, 0.0274449494, 0.0542896502, -0.0406049564, 0.0361476652, -0.0402631424, 0.0063131908, 0.0775951073, -0.0519786440, 0.0436651148, -0.1053666472, -0.0236011874, -0.0663688630, 0.0245846380, -0.0121529764, 0.0412541144, -0.0329084396, 0.0424758457, 0.0702285990, -0.0032417171, -0.0383609422, 0.0131835435, 0.0103320954};
fc1_bias_t b2[20] = {-0.0046787495, -0.0381353125, -0.0392166786, -0.0362958163, -0.0221129712, 0.0103091607, -0.0120259160, -0.0555078387, 0.0126220537, -0.0571111105, 0.0392405428, 0.0045202081, -0.0132382354, 0.0357641801, 0.0216694009, 0.1329858154, 0.0643487871, -0.0385710187, -0.0095817717, -0.0649708807};

fc2_weight_t w4[100] = {-0.0585213862, -0.0474466309, 0.0212179199, -0.0436153784, -0.0273020212, -0.0371719562, 0.0547070019, 0.0165620390, 0.0317363851, -0.0233268067, -0.0929842666, -0.0434811562, -0.0142044071, -0.0637552738, 0.0283018947, 0.0436815023, 0.0782769918, -0.0021439490, -0.0685177520, 0.0183066186, -0.0957280993, 0.0808159038, -0.0240277946, -0.0397360139, 0.0587152652, 0.0456780158, 0.0670054257, 0.0044776946, -0.0380984955, -0.0902777240, 0.0085247485, -0.1048997194, -0.0174131207, 0.0434048474, 0.0428164303, -0.0949964747, 0.0155735882, -0.0538620763, 0.0329079591, -0.0386982560, 0.0148765510, 0.0196018089, 0.0218994394, 0.0791911632, -0.0399679951, 0.0146347405, 0.0666100532, -0.0555019490, 0.0086978991, 0.0346531421, 0.0954504237, 0.0146730235, -0.0610129349, -0.0340039544, 0.0365986377, -0.1219084188, -0.0585061125, -0.0135962740, 0.0822919905, -0.0085318349, -0.0267830323, 0.0435298048, 0.0377885886, 0.0307887997, 0.0174705312, 0.0107315993, 0.0202817786, -0.0215425752, 0.0245208852, -0.0358954892, 0.0353907794, -0.0063549639, -0.1054334268, -0.0347316563, 0.0710420012, 0.1029884145, 0.0282584075, 0.0044269259, 0.0083026746, 0.0272888485, 0.0132943690, 0.0109224115, -0.0326045118, 0.0045732348, 0.0422399007, -0.1294411123, 0.1085677147, 0.0035562781, 0.0340289325, -0.0148089407, -0.0324783251, 0.0169265456, 0.1563816220, -0.0095088836, 0.0424668603, -0.0249741450, -0.0449088626, 0.0054188566, 0.0015064594, -0.0625505075};
fc2_bias_t b4[5] = {-0.0048617339, -0.0740748346, 0.0318292938, 0.0038419308, 0.0494572334};

// Weights end

template<class T, int N>
struct channel_array {
   T arr[N]; 
};

void CCS_BLOCK(layer1)(
    input_t in[N_INPUT_1_1],
    ac_channel<channel_array<layer2_t, N_LAYER_2> > &out
) {
    channel_array<layer2_t, N_LAYER_2> layer2_out;
    nnet::dense<input_t, layer2_t, config2>(in, layer2_out.arr, w2, b2);
    
    out.write(layer2_out);
}

void CCS_BLOCK(layer2)(
    ac_channel<channel_array<layer2_t, N_LAYER_2> > &in,
    ac_channel<channel_array<layer3_t, N_LAYER_2> > &out
) {
    channel_array<layer2_t, N_LAYER_2> layer_in = in.read();
    channel_array<layer3_t, N_LAYER_2> layer_out;
    
    nnet::relu<layer2_t, layer3_t, relu_config3>(layer_in.arr, layer_out.arr);
    
    out.write(layer_out);
}

void CCS_BLOCK(layer3)(
    ac_channel<channel_array<layer3_t, N_LAYER_2> > &in,
    ac_channel<channel_array<result_t, N_LAYER_4> > &out
) {
    channel_array<layer3_t, N_LAYER_2> layer_in = in.read();
  
    channel_array<layer4_t, N_LAYER_4> layer4_out;
    nnet::dense<layer3_t, layer4_t, config4>(layer_in.arr, layer4_out.arr, w4, b4);
    
    out.write(layer4_out);
}

void CCS_BLOCK(layer4)(
    ac_channel<channel_array<layer4_t, N_LAYER_4> > &in,
    result_t out[N_LAYER_4],
    unsigned short &const_size_in_1,
    unsigned short &const_size_out_1
) {
    channel_array<layer4_t, N_LAYER_4> layer_in = in.read();
    
    nnet::softmax<layer4_t, result_t, softmax_config5>(layer_in.arr, out);

    const_size_in_1 = N_INPUT_1_1;
    const_size_out_1 = N_LAYER_4;
}

void myproject(
    input_t fc1_input[N_INPUT_1_1],
    result_t layer5_out[N_LAYER_4],
    unsigned short &const_size_in_1,
    unsigned short &const_size_out_1
) {

    //hls-fpga-machine-learning insert IO
    //#pragma HLS ARRAY_RESHAPE variable=fc1_input complete dim=0
    //#pragma HLS ARRAY_PARTITION variable=layer5_out complete dim=0
    //#pragma HLS INTERFACE ap_vld port=fc1_input,layer5_out 
    //#pragma HLS PIPELINE

    const_size_in_1 = N_INPUT_1_1;
    const_size_out_1 = N_LAYER_4;

    // ****************************************
    // NETWORK INSTANTIATION
    // ****************************************

    //hls-fpga-machine-learning insert layers
    
    // layer2_t layer2_out[N_LAYER_2];
    // nnet::dense<input_t, layer2_t, config2>(fc1_input, layer2_out, w2, b2); // fc1

    // layer3_t layer3_out[N_LAYER_2];
    // nnet::relu<layer2_t, layer3_t, relu_config3>(layer2_out, layer3_out); // fc1_relu

    // layer4_t layer4_out[N_LAYER_4];
    // nnet::dense<layer3_t, layer4_t, config4>(layer3_out, layer4_out, w4, b4); // fc2

    // nnet::softmax<layer4_t, result_t, softmax_config5>(layer4_out, layer5_out); // fc2_softmax
    
    static ac_channel<channel_array<layer2_t, N_LAYER_2> > layer2_out;
    static ac_channel<channel_array<layer3_t, N_LAYER_2> > layer3_out;
    static ac_channel<channel_array<layer4_t, N_LAYER_4> > layer4_out;
    
    layer1(fc1_input, layer2_out);
    layer2(layer2_out, layer3_out);
    layer3(layer3_out, layer4_out);
    layer4(layer4_out, layer5_out, const_size_in_1, const_size_out_1);
}
